{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier for Hand-Written Digits Recognition\n",
    "\n",
    "This program classifies images of hand-written digits in MNIST dataset using logistic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../Datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Datasets/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read mnist dataset\n",
    "mnist = input_data.read_data_sets(\"../Datasets/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 784\n",
    "n_labels = 10\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "learn_rate = 0.003\n",
    "\n",
    "features = tf.placeholder(\"float\", shape=([None, n_features]), name = 'features')\n",
    "labels = tf.placeholder(\"float\", shape=([None, n_labels]), name = 'labels')\n",
    "weights = tf.Variable(tf.random_normal(shape=([n_features, n_labels])), name = 'weights')\n",
    "biases = tf.Variable(tf.zeros(n_labels), name = 'biases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Classifier\n",
    "A logistic classifier is a linear classifier, it takes inputs, which are pixels in this problem, and applies linear function (xW + b) to them to generate predictions. The output of linear function are called logits, and they represent the score for each class in the dataset. The scores are converted to probability using Softmax function. Further, the labels in the dataset are one-hot encoded. In order to compare the probabilites with the one-hot encoded labels, we use cross-entropy. A cross-entropy, in layman terms, measures distance between the probabilies and the one hot-encoded values. Once we find the distance between predicted value and the actual value, we calculate the loss as mean of cross-entropy over entire dataset. The loss is minimized and correct weights and bias are learned using Gradient-Descent optimizer.\n",
    "\n",
    "### Why am I using Softmax function?\n",
    "Softmax function is a non-linear function and it converts a linear input to non-linear output. This is important in multi-layer neural network because if we don't use a non-linear activation function, then no matter how deep our neural network is, it can basically be represented by a single layer neural network (because linear combination of linear combinations is again a linear combination), and we gain nothing from adding multiple layers. So in order to take benefit of multiple layers in a neural network, we use non-linear activation functions like softmax, ReLU, tanh, sigmoid etc... And most of the real data available is not linearly separable, we need a non-linear model to model such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.add(tf.matmul(features, weights), biases)\n",
    "\n",
    "# Below 3 lines can be written with a single command: tf.nn.softmax_cross_entropy_with_logits()\n",
    "softmax = tf.nn.softmax(logits = logits)\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(labels, tf.log(softmax)))\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Using Gradient Descent to learn correct weights and biases\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learn_rate).minimize(loss)\n",
    "\n",
    "# Calculate prediction accuracy\n",
    "correct_predictions = tf.equal(tf.argmax(logits, axis = 1), tf.argmax(labels, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  10 , Validation Accuracy =  0.8448\n",
      "Epoch =  20 , Validation Accuracy =  0.9034\n",
      "Epoch =  30 , Validation Accuracy =  0.9112\n",
      "Epoch =  40 , Validation Accuracy =  0.9164\n",
      "Epoch =  50 , Validation Accuracy =  0.9184\n",
      "Epoch =  60 , Validation Accuracy =  0.9168\n",
      "Epoch =  70 , Validation Accuracy =  0.9216\n",
      "Epoch =  80 , Validation Accuracy =  0.924\n",
      "Epoch =  90 , Validation Accuracy =  0.9262\n",
      "Epoch =  100 , Validation Accuracy =  0.9228\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize tf Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train over n_epochs\n",
    "    for e in range(n_epochs):\n",
    "        # Divide dataset into batches and train the model over the batches for each epoch\n",
    "        num_batches = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(num_batches):\n",
    "            features_batch, labels_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, \n",
    "                     feed_dict = {\n",
    "                                     features : features_batch, \n",
    "                                     labels : labels_batch\n",
    "                                 })\n",
    "        \n",
    "        # Print learning progress for every 10 epochs\n",
    "        if(e%10 == 0):\n",
    "            output = sess.run(accuracy, \n",
    "                              feed_dict={\n",
    "                                          features : mnist.validation.images, \n",
    "                                          labels : mnist.validation.labels\n",
    "                                        })\n",
    "            print(\"Epoch = \", e+10, \", Validation Accuracy = \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
